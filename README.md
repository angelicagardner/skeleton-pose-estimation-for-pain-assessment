# Spot the Pain: Exploring the Application of Skeleton Pose Estimation for Automated Pain Assessment

![License](https://img.shields.io/badge/license-MIT-38bdf8?style=flat-square)

This repository contains the code implementation for [my Master's Thesis project](https://www.diva-portal.org/smash/record.jsf%3Fpid%3Ddiva2:1673390) at Linnaeus University (2022).

The research explores how body gestures (skeleton data) can be used as a primary or complementary modality to facial expressions for objective pain recognition and localization.

**ðŸ“„ [Read the full thesis (PDF)](./docs/spot_the_pain_thesis.pdf)**

---

## Abstract

Automated pain assessment is an essential part of modern healthcare and rehabilitation. While most research focuses on facial expressions, this study investigates the understudied modality of body gestures.

By using skeleton pose estimation, we modeled body movements and analyzed their impact on:

1. Pain Recognition (Detection)
2. Pain Intensity Estimation (Levels)
3. Pain Area Classification (Localization)

Through unimodal (body-only) and bimodal (body + face) approaches using **CNN-BiLSTM** and **RCNN** architectures, we demonstrated the feasibility of using skeleton data to enhance automated assessment.

## Project Organization

    â”œâ”€â”€ Makefile           <- Makefile with commands like `make data` or `make train`
    â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
    â”œâ”€â”€ data
    â”‚Â Â  â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.
    â”‚Â Â  â””â”€â”€ raw            <- The original, immutable data dump.
    â”‚
    â”œâ”€â”€ models             <- Trained and serialized models or script files to download models from GitHub repositories
    â”‚
    â”œâ”€â”€ notebooks          <- Jupyter notebooks
    â”‚
    â”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    â”‚Â Â  â””â”€â”€ architectures  <- Generated graph plots of the deep learning models
    â”‚Â Â  â””â”€â”€ figures        <- Generated graphics and figures to be used in reporting
    â”‚
    â”œâ”€â”€ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    â”‚                         generated with `pip freeze > requirements.txt`
    â”‚
    â”œâ”€â”€ setup.py           <- Makes project pip installable (pip install -e .) so src can be imported
    â”œâ”€â”€ src                <- Source code for use in this project.
    â”‚Â Â  â”œâ”€â”€ data           <- Scripts to download or generate data
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ make_dataset.py
    â”‚Â Â  â”‚Â Â  â””â”€â”€ load_dataset.py    
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ lib             <- External libraries used in this project
    â”‚Â Â  â”‚Â Â  â””â”€â”€ DeepStack
    â”‚Â Â  â”‚Â   â””â”€â”€ time_series_augmentation
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ models         <- Python classes to use for training models and save to make predictions

## ðŸ› System Architecture & Fusion Strategies

The project investigated how to best combine facial features (Action Units) with body keypoints.

```mermaid
---
title: Fusion Pipeline
---
graph TD
    subgraph Modalities
        A[Video Input] --> B[PoseNet: 17 Body Keypoints]
        A --> C[OpenFace: Facial Action Units]
    end

    subgraph Strategies
        B --> D{Fusion Logic}
        C --> D
        D -->|Early| E[Input Concatenation]
        D -->|Late| F[Score Averaging]
        D -->|Ensemble| G[Weighted Model Voting]
    end

    E --> H[Deep Learning Model]
    F --> H
    G --> H
    H --> I[Recognition / Intensity / Area]
```

## ðŸ›  Tech Stack (2022)

- **Pose Estimation**: [PoseNet](https://github.com/tensorflow/tfjs-models/tree/master/posenet) (17 keypoints + confidence scores).
- **Facial Analysis**: [OpenFace](https://github.com/cmusatyalab/openface) (Action Units for PSPI measures).
- **Architectures**: Hybrid CNN-BiLSTM and Recurrent CNN (RCNN).
- **Data Engineering**: [Cookiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/) for reproducible project structure.

### Pain Intensity Metric (PSPI)

The system aimed to approximate the Prkachin and Solomon Pain Intensity (PSPI) scale:

$$PSPI \approx AU4 + (AU6 \lor AU7) + (AU9 \lor AU10) + AU43$$

> *(Note: AU43 was omitted due to OpenFace limitations in the 2022 setup).*

## ðŸ“Š Experimental Results

The models were trained on a private dataset of video recordings featuring overhead deep squat exercises. The dataset consisted of 6-second video recordings with one person performing a overhead deep squat. The whole body and face are visible, the person looks into the camera from the front. The file ```make_dataset.py``` was run once to load the CSV file output by PoseNet and OpenFace to create the final dataset used in this project.

| Objective | Best Strategy | Metric (AUC) |
| :--- | :--- | :--- |
| Pain Recognition | Bimodal Ensemble | $0.71$ |
| Intensity Estimation | Unimodal Body (CNN-BiLSTM) | $0.75$ |
| Area Classification | Late Fusion (RCNN) | $0.75$ |

## ðŸš€ Reflections (2026 Perspective)

Looking back at this research four years later, the landscape of Pose Estimation and Multimodal Learning has evolved significantly. If I were to iterate on this today:

- From CNN-BiLSTM to Graph Neural Networks (GNNs): Skeleton data is inherently a graph. Today, I would use ST-GNNs (Spatio-Temporal Graph Neural Networks) to better capture the anatomical dependencies between joints.
- Transformers: I would explore Video Vision Transformers (ViT) for the bimodal fusion, using cross-attention mechanisms to let the model "decide" which modality (face or body) is more reliable for a specific frame.
- Real-time Edge Deployment: With current hardware, the PoseNet-based inference could be moved entirely to the edge (e.g., using Wasm or CoreML) for real-time clinical feedback.

---

## License & Dataset Disclaimer

This implementation code is licensed under the [MIT License](LICENSE). You are free to use, modify, and distribute the code for academic or professional purposes, provided that appropriate credit is given.

**Important:** The dataset used in this research (video recordings of overhead deep squats) is **private** and is **not included** in this repository. 

- Due to ethical constraints and privacy agreements with the participants, the raw video data and processed CSV files cannot be made public.
- This repository is provided for architectural reference and transparency regarding the models and methodologies used in the thesis.

## Changelog

**[July 2022]** â€“ Thesis published on DiVA.

**[Feb 2026]** â€“ Added thesis as PDF and updated README, then archived repository.

## Citation

```
@misc{gardner2022spotthepain,
  author       = {Angelica Hjelm Gardner},
  title        = {Spot the Pain: Exploring the Application of Skeleton Pose Estimation for Automated Pain Assessment},
  howpublished = {Master's Thesis, Linnaeus University},
  year         = {2022}
}
```
